# ğŸ”‘ Enhanced BYOK (Bring Your Own Key) AI Template

**Complete, production-ready template for multi-provider AI integration with embedded models, local AI support, and enhanced model selection.**

> *Based on the proven neurospark educational platform - now enhanced with support for 5+ AI providers, embedded models, and local AI capabilities.*

## ğŸŒŸ What's New in the Enhanced Version

### ğŸš€ **Multi-Provider Support**
- **Google Gemini** - Latest models including Gemini 2.0 Flash
- **Groq** - Ultra-fast inference with Llama, Gemma, and Phi models
- **HuggingFace** - Access to open-source models via Inference API
- **Ollama** - Local AI models running on your machine
- **OpenRouter** - Gateway to multiple AI providers

### ğŸ¯ **Enhanced Model Selection**
- Choose specific models for each provider
- Embedded models: Gemma 2, Phi-3, Llama 3.2/3.3
- Model descriptions and strength indicators
- Context length and capability information

### ğŸ  **Local + Cloud Hybrid**
- Run models locally with Ollama (privacy-first)
- Mix local and cloud providers seamlessly
- No API costs for local models
- Offline capability when using local models

## ğŸ® Features

### âœ¨ **Core Features**
- ğŸ” **Pure Client-Side Security** - API keys never leave your browser
- ğŸ”„ **Provider Switching** - Switch between providers instantly
- ğŸ¯ **Model Selection** - Choose the best model for each task
- ğŸ’¬ **Chat & Generation** - Both single-shot and conversational AI
- ğŸ  **Local AI Support** - Privacy-first local model execution
- ğŸ“± **Modern UI** - Beautiful, responsive interface with Tailwind CSS

### ğŸ›  **Technical Features**
- âš›ï¸ React 18 + TypeScript + Vite
- ğŸ”¥ Firebase Authentication (Google OAuth)
- ğŸ’¾ localStorage-based key management
- ğŸ¨ Tailwind CSS styling
- ğŸ“¦ Zero server dependencies

## ğŸš€ Quick Start

### 1. Clone and Install
```bash
git clone https://github.com/satishskid/BYOK-template.git
cd BYOK-template
npm install
```

### 2. Firebase Setup
1. Create a Firebase project at [Firebase Console](https://console.firebase.google.com)
2. Enable Google Authentication
3. Copy your config to `src/firebaseConfig.js`

### 3. Start Development
```bash
npm run dev
```

### 4. Configure Providers
1. Sign in with Google
2. Go to "Setup" tab
3. Add API keys for your preferred providers
4. Select models for each provider
5. Start using AI!

## ğŸ”§ Supported Providers & Models

### â˜ï¸ **Cloud Providers**

#### Google Gemini (Free Tier Available)
- **Gemini 2.0 Flash (Experimental)** - Latest with enhanced capabilities
- **Gemini 1.5 Pro** - Complex reasoning and long context
- **Gemini 1.5 Flash** - Fast and efficient for most tasks
- ğŸ†“ 60 requests/minute free

#### Groq (Fast Inference)
- **Llama 3.3 70B** - Most capable with excellent reasoning
- **Llama 3.1 8B Instant** - Fast and efficient responses
- **Gemma 2 9B** - Google's open model optimized for instructions
- **Phi-3 Medium** - Microsoft's compact but powerful model
- ğŸ†“ Generous free tier

#### HuggingFace (Open Source Hub)
- **Llama 3.2 3B Instruct** - Compact instruction-tuned model
- **Phi-3.5 Mini** - Microsoft's efficient instruction model  
- **Gemma 2 2B** - Google's lightweight safety-focused model
- ğŸ†“ Free inference API

#### OpenRouter (Multi-Provider Gateway)
- **Gemini Flash 1.5** - Google's fast model via OpenRouter
- **Claude 3 Haiku** - Anthropic's efficient model
- **Llama 3.1 8B (Free)** - Free tier access to Llama
- ğŸ†“ Free tier available

### ğŸ  **Local Provider**

#### Ollama (Privacy-First)
- **Llama 3.2** - Meta's latest model running locally
- **Phi 3.5** - Microsoft's efficient local model
- **Gemma 2** - Google's open model for local deployment
- **CodeLlama** - Specialized for code generation
- ğŸ”’ 100% private, no API costs, offline capable

## ğŸ¯ Use Cases & Model Recommendations

### ğŸ’¼ **Business & Professional**
- **Complex Analysis**: Gemini 1.5 Pro, Llama 3.3 70B
- **Quick Tasks**: Gemini Flash, Llama 3.1 8B Instant
- **Cost-Sensitive**: HuggingFace models, Ollama local

### ğŸ‘¨â€ğŸ’» **Development & Code**
- **Code Generation**: CodeLlama (local), Phi-3 models
- **Code Review**: Gemini 1.5 Pro, Llama 3.3 70B
- **Quick Snippets**: Phi-3.5 Mini, Gemma 2

### ğŸ”’ **Privacy-Sensitive**
- **Confidential Data**: Ollama local models
- **Personal Projects**: Any local Ollama model
- **Offline Work**: Ollama (works without internet)

### ğŸ“š **Learning & Education**
- **Explanations**: Gemini models (great at teaching)
- **Practice**: Free tier models (Groq, HuggingFace)
- **Experimentation**: Ollama local models

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT-SIDE ONLY                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Firebase Auth  â”‚  localStorage Keys  â”‚  Direct AI Calls    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Google    â”‚â”‚  â”‚ Provider Keys   â”‚â”‚  â”‚   Multi-AI      â”‚â”‚
â”‚  â”‚   Sign-In   â”‚â”‚  â”‚ Model Settings  â”‚â”‚  â”‚   Providers     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚ User Prefs      â”‚â”‚  â”‚   â€¢ Gemini      â”‚â”‚
â”‚                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚   â€¢ Groq        â”‚â”‚
â”‚                 â”‚                     â”‚  â”‚   â€¢ HuggingFace â”‚â”‚
â”‚                 â”‚                     â”‚  â”‚   â€¢ Ollama      â”‚â”‚
â”‚                 â”‚                     â”‚  â”‚   â€¢ OpenRouter  â”‚â”‚
â”‚                 â”‚                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ File Structure

```
src/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ aiService.ts              # ğŸ§  Enhanced multi-provider AI service
â”‚   â””â”€â”€ firebaseService.ts        # ğŸ” Authentication wrapper
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ EnhancedApiKeyManager.tsx # âš™ï¸  Multi-provider + model management
â”‚   â”œâ”€â”€ AIPlayground.tsx          # ğŸ® Interactive AI testing interface
â”‚   â”œâ”€â”€ ApiKeyWarning.tsx         # âš ï¸  Missing key notifications
â”‚   â””â”€â”€ SettingsView.tsx          # ğŸ›ï¸  Settings modal
â”œâ”€â”€ App.tsx                       # ğŸ  Enhanced main app with tabs
â””â”€â”€ firebaseConfig.js             # ğŸ”¥ Firebase configuration
```

## ğŸ”§ Configuration Examples

### Adding a New Provider
```typescript
// In aiService.ts - Add to PROVIDERS_CONFIG
newprovider: {
  name: "New Provider",
  description: "Description of capabilities",
  keyPrefix: "np_",
  testPrompt: "Test prompt",
  requiresKey: true,
  setupUrl: "https://newprovider.com",
  models: [
    {
      id: "model-id",
      name: "Model Name",
      description: "What this model is good at",
      strengths: ["Strength 1", "Strength 2"],
      isDefault: true
    }
  ]
}
```

### Custom Model Selection
```typescript
// Switch provider and model
setAIConfig('groq', 'your-api-key', 'llama-3.3-70b-versatile');

// Generate with specific provider
const response = await generateContent("Your prompt");
```

## ğŸš€ Deployment

### Environment Variables
**None required!** This is a pure BYOK system - all configuration is client-side.

### Build & Deploy
```bash
# Build for production
npm run build

# Deploy to Netlify, Vercel, or any static host
npm run preview
```

### Netlify Configuration
```toml
# netlify.toml
[build]
  command = "npm run build"
  publish = "dist"

[[redirects]]
  from = "/*"
  to = "/index.html"
  status = 200
```

## ğŸ”’ Security & Privacy

### ğŸ›¡ï¸ **Security Features**
- API keys stored only in browser localStorage
- Direct API calls from browser to providers
- No server-side key storage or proxying
- Firebase Auth for user management only

### ğŸ” **Privacy Options**
- **Full Privacy**: Use Ollama local models only
- **Hybrid**: Mix local models for sensitive tasks, cloud for others
- **Cloud**: Use any combination of cloud providers

### ğŸš¨ **Best Practices**
- Use local models (Ollama) for confidential data
- Rotate API keys regularly
- Use free tiers for experimentation
- Mix providers based on task requirements

## ğŸ“š Usage Examples

### Basic AI Request
```typescript
import { generateContent } from './services/aiService';

const response = await generateContent('Explain quantum computing');
console.log(response);
```

### Chat Conversation
```typescript
import { getChatResponse } from './services/aiService';

const messages = [
  { role: 'user', content: 'Hello!' },
  { role: 'assistant', content: 'Hi there!' },
  { role: 'user', content: 'How are you?' }
];

const response = await getChatResponse(messages);
```

### Provider Management
```typescript
import { setAIConfig, switchProvider } from './services/aiService';

// Switch to Groq with specific model
setAIConfig('groq', 'gsk_your_key', 'llama-3.3-70b-versatile');

// Test if Ollama is running locally
const isOllamaReady = await testApiKey('ollama', '');
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test with multiple providers
5. Submit a pull request

## ğŸ“„ License

MIT License - feel free to use this template in your projects!

## ğŸ™ Acknowledgments

- Based on the neurospark educational platform
- Inspired by the BYOK (Bring Your Own Key) philosophy
- Built with modern React, TypeScript, and Vite
- UI powered by Tailwind CSS

---

**Ready to build with AI? Clone this template and start creating! ğŸš€**
